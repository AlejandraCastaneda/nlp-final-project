{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary libraries to use BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "# import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to set up the dataset so that it can be used with a TensorFlow model (i.e. BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/alejc/onedrive/Desktop/CS159/nlp-final-project/data/liar.data/train.tsv\", \"r\", encoding=\"utf-8\") as infile, open(\"/Users/alejc/onedrive/Desktop/CS159/nlp-final-project/data/liar.data/traincleaned.tsv\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        # Remove problematic quotes and extra line breaks\n",
    "        clean_line = line.replace('\"', '').replace('\\n', '').replace('\\r', '')\n",
    "        outfile.write(clean_line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tsv_line(line):\n",
    "    # Define the column types\n",
    "    column_defaults = [\n",
    "    '',  # JSON file name\n",
    "    '',  # Boolean flag as string\n",
    "    '',  # Statement\n",
    "    '',  # Topic\n",
    "    '',  # Speaker\n",
    "    '',  # Speaker's title\n",
    "    '',  # State\n",
    "    '',  # Party\n",
    "    0,   # Truth count 1\n",
    "    0,   # Truth count 2\n",
    "    0,   # Truth count 3\n",
    "    0,   # Truth count 4\n",
    "    0,   # Truth count 5\n",
    "    ''   # Source\n",
    "]\n",
    "    # Decode the line into individual columns\n",
    "    columns = tf.io.decode_csv(line, record_defaults=column_defaults, field_delim='\\t', use_quote_delim=True)\n",
    "    # Separate features and label\n",
    "    # Separate features and label (assuming no label for now)\n",
    "    string_features = columns[2:8] + columns[-1:]  # Select all string columns\n",
    "    numeric_features = columns[8:-1]  # Select integer columns (truth counts)\n",
    "\n",
    "    # Combine string and numeric features into a single dictionary\n",
    "    features = {\n",
    "        \"string_features\": string_features,\n",
    "        \"numeric_features\": tf.stack(numeric_features)\n",
    "    }\n",
    "\n",
    "    # If there's a label column, define it here; otherwise, return features only\n",
    "    label = columns[1]  # Replace with actual label column index if applicable\n",
    "    return features, label\n",
    "\n",
    "def create_tsv_dataset(file_path, batch_size=32):\n",
    "    # Load the file\n",
    "    dataset = tf.data.TextLineDataset(file_path)\n",
    "    # Skip the header if the file has one\n",
    "    # dataset = dataset.skip(1) <- curret file doesn't have a header but if it did we could skip the top row\n",
    "    # Parse each line\n",
    "    dataset = dataset.map(parse_tsv_line)\n",
    "    # Shuffle, batch, and prefetch for performance\n",
    "    dataset = dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your .tsv file\n",
    "file_path = \"/Users/alejc/onedrive/Desktop/CS159/nlp-final-project/data/liar.data/traincleaned.tsv\"\n",
    "\n",
    "# Create the dataset\n",
    "batch_size = 32\n",
    "\n",
    "dataset = create_tsv_dataset(file_path, batch_size)\n",
    "\n",
    "# Iterate through the dataset\n",
    "for features, labels in dataset.take(1):\n",
    "    print(\"Features:\", features)\n",
    "    print(\"Labels:\", labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize pandas to get some preliminary data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the TSV file into a pandas DataFrame\n",
    "file_path = \"/Users/alejc/onedrive/Desktop/CS159/nlp-final-project/data/liar.data/train.tsv\"\n",
    "column_names = [\n",
    "    \"json_file\", \"label\", \"statement\", \"topic\", \"speaker\", \"speaker_title\",\n",
    "    \"state\", \"party\", \"barely_true\", \"false\", \"half_true\",\n",
    "    \"mostly_true\", \"pants_on_fire\", \"source\"\n",
    "]\n",
    "df = pd.read_csv(file_path, sep='\\t', names=column_names)\n",
    "\n",
    "# Find the most common party\n",
    "most_common_label = df['party'].value_counts().idxmax()\n",
    "print(\"Most common label:\", most_common_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by label and compute the mean of truth_count_5\n",
    "average_truth_count_5_per_label = df.groupby('state')['pants_on_fire'].mean()\n",
    "print(\"Average Truth Count 5 per label:\")\n",
    "print(average_truth_count_5_per_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by label and state, and count occurrences\n",
    "state_counts = df.groupby(['label', 'state']).size().reset_index(name='count')\n",
    "\n",
    "# For each label, find the top 5 states\n",
    "top_5_states_by_label = state_counts.groupby('label').apply(\n",
    "    lambda group: group.nlargest(5, 'count')\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"Top 5 states by label:\")\n",
    "print(top_5_states_by_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data For BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read train data\n",
    "train = pd.read_csv(\"/Users/alejc/onedrive/Desktop/CS159/nlp-final-project/data/liar.data/train.tsv\", sep='\\t', header=None)\n",
    "\n",
    "# Drop columns by index\n",
    "columns_to_remove = [0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]  # Indices of the columns to remove\n",
    "train = train.drop(columns=columns_to_remove)\n",
    "\n",
    "# assign numbers to truth label\n",
    "label_column = train.columns[0]  \n",
    "# Define the mapping of strings to numbers\n",
    "label_mapping = {\n",
    "    'true': 0,\n",
    "    'mostly-true': 1,\n",
    "    'half-true': 2,\n",
    "    'barely-true': 3,\n",
    "    'false': 4,\n",
    "    'pants-fire': 5\n",
    "}\n",
    "train[label_column] = train[label_column].map(label_mapping)\n",
    "\n",
    "# Save the updated DataFrame back to a TSV file\n",
    "output_path = 'train_update.tsv'  # Replace with your desired output file path\n",
    "train.to_csv(output_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data\n",
    "test = pd.read_csv(\"/Users/alejc/onedrive/Desktop/CS159/nlp-final-project/data/liar.data/test.tsv\", sep='\\t', header=None)\n",
    "\n",
    "# Drop columns by index\n",
    "columns_to_remove = [0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]  # Indices of the columns to remove\n",
    "test = test.drop(columns=columns_to_remove)\n",
    "\n",
    "# assign numbers to truth label\n",
    "label_column = test.columns[0]  \n",
    "# Define the mapping of strings to numbers\n",
    "label_mapping = {\n",
    "    'true': 0,\n",
    "    'mostly-true': 1,\n",
    "    'half-true': 2,\n",
    "    'barely-true': 3,\n",
    "    'false': 4,\n",
    "    'pants-fire': 5\n",
    "}\n",
    "test[label_column] =  test[label_column].map(label_mapping)\n",
    "\n",
    "# Save the updated DataFrame back to a TSV file\n",
    "output_path = 'test_update.tsv'  # Replace with your desired output file path\n",
    "test.to_csv(output_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.read_csv(\"/Users/alejc/onedrive/Desktop/CS159/nlp-final-project/data/liar.data/train_update.tsv\", sep='\\t', header=None)\n",
    "test = pd.read_csv(\"/Users/alejc/onedrive/Desktop/CS159/nlp-final-project/data/liar.data/test_update.tsv\", sep='\\t', header=None)\n",
    "\n",
    "\n",
    "# Create dataframes for train \n",
    "train_bert_df = pd.DataFrame({\n",
    "    'id': range(len(train)),\n",
    "    'label': train[0],\n",
    "    'alpha': ['q']*train.shape[0],\n",
    "    'text': train[1].replace(r'\\n', ' ', regex=True).str.lower()\n",
    "})\n",
    "\n",
    "test_bert_df = pd.DataFrame({\n",
    "    'id': range(len(test)),\n",
    "    'text': test[1].replace(r'\\n', ' ', regex=True).str.lower()\n",
    "})\n",
    "\n",
    "train_bert_df.head()\n",
    "\n",
    "train_bert_df.to_csv('bert_df_train.tsv', sep='\\t', index=False, header=False)\n",
    "test_bert_df.to_csv('bert_df_test.tsv', sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['python', 'C:\\\\Users\\\\alejc\\\\OneDrive\\\\Desktop\\\\CS159\\\\nlp-final-project\\\\code\\\\bert-master\\\\run_classifier.py', '--task_name=cola', '--do_train=true', '--do_predict=true', '--data_dir=C:\\\\Users\\\\alejc\\\\OneDrive\\\\Desktop\\\\CS159\\\\nlp-final-project\\\\data\\\\liar.data\\\\bert_df_train.tsv', '--vocab_file=C:\\\\Users\\\\alejc\\\\OneDrive\\\\Desktop\\\\CS159\\\\nlp-final-project\\\\code\\\\BERT\\\\vocab.txt', '--bert_config_file=C:\\\\Users\\\\alejc\\\\OneDrive\\\\Desktop\\\\CS159\\\\nlp-final-project\\\\code\\\\BERT\\\\bert_config.json', '--init_checkpoint=C:\\\\Users\\\\alejc\\\\OneDrive\\\\Desktop\\\\CS159\\\\nlp-final-project\\\\code\\\\BERT\\\\bert_model.ckpt', '--max_seq_length=128', '--output_dir=C:\\\\Users\\\\alejc\\\\OneDrive\\\\Desktop\\\\CS159\\\\nlp-final-project\\\\code\\\\model_output']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 26\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Define the command as a list of arguments\u001b[39;00m\n\u001b[0;32m     13\u001b[0m command \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m, run_classifier_path,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--task_name=cola\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Update this to match your task if different\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--output_dir=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m ]\n\u001b[1;32m---> 26\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mstdout)  \u001b[38;5;66;03m# Print the standard output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py:528\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    526\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[1;32m--> 528\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[0;32m    529\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['python', 'C:\\\\Users\\\\alejc\\\\OneDrive\\\\Desktop\\\\CS159\\\\nlp-final-project\\\\code\\\\bert-master\\\\run_classifier.py', '--task_name=cola', '--do_train=true', '--do_predict=true', '--data_dir=C:\\\\Users\\\\alejc\\\\OneDrive\\\\Desktop\\\\CS159\\\\nlp-final-project\\\\data\\\\liar.data\\\\bert_df_train.tsv', '--vocab_file=C:\\\\Users\\\\alejc\\\\OneDrive\\\\Desktop\\\\CS159\\\\nlp-final-project\\\\code\\\\BERT\\\\vocab.txt', '--bert_config_file=C:\\\\Users\\\\alejc\\\\OneDrive\\\\Desktop\\\\CS159\\\\nlp-final-project\\\\code\\\\BERT\\\\bert_config.json', '--init_checkpoint=C:\\\\Users\\\\alejc\\\\OneDrive\\\\Desktop\\\\CS159\\\\nlp-final-project\\\\code\\\\BERT\\\\bert_model.ckpt', '--max_seq_length=128', '--output_dir=C:\\\\Users\\\\alejc\\\\OneDrive\\\\Desktop\\\\CS159\\\\nlp-final-project\\\\code\\\\model_output']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define file paths\n",
    "run_classifier_path = r\"C:\\Users\\alejc\\OneDrive\\Desktop\\CS159\\nlp-final-project\\code\\bert-master\\run_classifier.py\"\n",
    "bert_config_path = r\"C:\\Users\\alejc\\OneDrive\\Desktop\\CS159\\nlp-final-project\\code\\BERT\\bert_config.json\"\n",
    "bert_checkpoint_prefix = r\"C:\\Users\\alejc\\OneDrive\\Desktop\\CS159\\nlp-final-project\\code\\BERT\\bert_model.ckpt\"\n",
    "vocab_path = r\"C:\\Users\\alejc\\OneDrive\\Desktop\\CS159\\nlp-final-project\\code\\BERT\\vocab.txt\"\n",
    "train_data_path = r\"C:\\Users\\alejc\\OneDrive\\Desktop\\CS159\\nlp-final-project\\data\\liar.data\\bert_df_train.tsv\"\n",
    "output_dir = r\"C:\\Users\\alejc\\OneDrive\\Desktop\\CS159\\nlp-final-project\\code\\model_output\"\n",
    "\n",
    "# Define the command as a list of arguments\n",
    "command = [\n",
    "    \"python\", run_classifier_path,\n",
    "    \"--task_name=cola\",  # Update this to match your task if different\n",
    "    \"--do_train=true\",  # Include training step if required\n",
    "    \"--do_predict=true\",\n",
    "    f\"--data_dir={train_data_path}\",\n",
    "    f\"--vocab_file={vocab_path}\",\n",
    "    f\"--bert_config_file={bert_config_path}\",\n",
    "    f\"--init_checkpoint={bert_checkpoint_prefix}\",\n",
    "    \"--max_seq_length=128\",\n",
    "    f\"--output_dir={output_dir}\"\n",
    "]\n",
    "\n",
    "result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "print(result.stdout)  # Print the standard output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
